{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a test for implementing Attention in Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant and Parameter\n",
    "device_type = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic structure of Attention:\n",
    "\n",
    "    B: batch size\n",
    "    N: sequence length\n",
    "    C: (raw) embedding dimension\n",
    "    d: (inner) embedding dimension\n",
    "    nh: num. of head, note C = d * nh\n",
    "    alpha: scaling factor, typically 1 / sqrt(d)\n",
    "    mQ, mK, mV, mO:         matrix (C, C),          embedding project matrix\n",
    "    x:                      matrix (B, N, C),       input tensor\n",
    "                            matrix (B, nh, N, d)        reshape\n",
    "    M:                      matrix (1, 1, N, N),    mask on attention scope, '-INF' for un-attended token\n",
    "    D_attn, D_out:          dropout layer\n",
    "\n",
    "Forward Pass:\n",
    "\n",
    "    Q := x @ mQ, K := x @ mK, V := x @ mV:            \n",
    "                            matrix (B, nh, N, d)        reshape\n",
    "    S := alpha * Q @ K^T:   matrix (B, nh, N, N)    \n",
    "    P := sigmoid(S * M + '-INF' * (1-M)):     \n",
    "                            matrix (B, nh, N, N)\n",
    "    O := D_attn(P) @ V:     matrix (B, nh, N, d)\n",
    "    y := D_out(O @ mO):     matrix (B, nh, N, d),   output tensor\n",
    "                            matrix (B, N, C)            reverse reshape\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic module class\n",
    "class Attention(nn.Module):\n",
    "    MAX_SEQ_LENGTH = 1024\n",
    "    def __init__(self, n_embd: int,\n",
    "                       n_head: int = 1,\n",
    "                       dropout_ratio: float = 0,\n",
    "                       **kwargs):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0, 'Embedding size MUST be multiple of N head'\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.d = self.n_embd // self.n_head\n",
    "        self.mQ = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.mK = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.mV = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.mO = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.alpha = 1 / (self.d ** .5)\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            torch.tril(\n",
    "                torch.ones(self.MAX_SEQ_LENGTH, self.MAX_SEQ_LENGTH),\n",
    "            ).view(1, 1, self.MAX_SEQ_LENGTH, self.MAX_SEQ_LENGTH)\n",
    "        )\n",
    "        self.D_attn = nn.Dropout(dropout_ratio)\n",
    "        self.D_out = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x, \n",
    "                      **kwargs):\n",
    "        '''\n",
    "        Input,\n",
    "            x           tensor(B, N, d)     input\n",
    "        Output,\n",
    "            y           tensor(B, N, d)     output\n",
    "        '''\n",
    "        B, N, C = x.size()\n",
    "        assert C == self.n_embd, 'Embedding dimension MUST match'\n",
    "\n",
    "        Q, K, V = self.mQ(x), self.mK(x), self.mV(x) \n",
    "        # reshape to (B, nh, N, d = C // nh)\n",
    "        Q = Q.reshape(B, N, self.n_head, -1).transpose(1, 2)\n",
    "        K = K.reshape(B, N, self.n_head, -1).transpose(1, 2)\n",
    "        V = V.reshape(B, N, self.n_head, -1).transpose(1, 2)\n",
    "        S = self.alpha * Q @ K.transpose(-2, -1)\n",
    "        S = S.masked_fill(self.mask[:,:,:N,:N] == 0, float('-inf'))\n",
    "        P = F.softmax(S, dim=-1)\n",
    "        P = self.D_attn(P)\n",
    "        O = P @ V \n",
    "        # reshape back to (B, N, C)\n",
    "        O = O.transpose(1, 2).reshape(B, N, C)\n",
    "        y = self.mO(O)\n",
    "        y = self.D_out(y)\n",
    "\n",
    "        return y \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100, 32])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = Attention(\n",
    "    n_embd=32,\n",
    "    n_head=4,\n",
    "    dropout_ratio=0,\n",
    ")\n",
    "\n",
    "x = torch.rand(64, 100, 32)\n",
    "\n",
    "attn(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f5833218766b48e6e35e4452ee875aac0e2188d05bbe5298f2c62b79f08b222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
